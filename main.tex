\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Trabajo Final},
            pdfauthor={Reinier Mujica},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Trabajo Final}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Reinier Mujica}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{13 de enero de 2019}


\begin{document}
\maketitle

\subsubsection{Inicialización}\label{inicializacion}

Limpiamos todas las variables del espacio de trabajo y el terminal de R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}014}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(knitr)}
\end{Highlighting}
\end{Shaded}

Cargamos el paquete de wooldridge y los datos

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(wooldridge)}
\KeywordTok{attach}\NormalTok{(bwght2)}
\end{Highlighting}
\end{Shaded}

Fijamos la semilla

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Limpiar los datos}\label{limpiar-los-datos}

Vamos a remover los valores NA de los datos

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos =}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(bwght2)}
\end{Highlighting}
\end{Shaded}

\subsubsection{División del conjunto de
datos}\label{division-del-conjunto-de-datos}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.size =}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{dim}\NormalTok{(datos)[}\DecValTok{1}\NormalTok{] }\OperatorTok{*}\StringTok{ }\FloatTok{0.7}\NormalTok{)}
\NormalTok{train =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(datos)[}\DecValTok{1}\NormalTok{], train.size)}
\NormalTok{test =}\StringTok{ }\OperatorTok{-}\NormalTok{train}
\NormalTok{datos.train =}\StringTok{ }\NormalTok{datos[train, ]}
\NormalTok{datos.test =}\StringTok{ }\NormalTok{datos[test, ]}
\NormalTok{test.size =}\StringTok{ }\KeywordTok{dim}\NormalTok{(datos.test)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Preguntas:}\label{preguntas}

\paragraph{a)}\label{a}

El conjunto de datos es bwght2 y contiene información sobre nacimientos.
Estos datos han sido obtenidos de certificados de nacimiento y
desfunción y también de información de natalidad y mortalidad del Centro
Nacional de Estadística para la Salud. El dataset contiene varias
variables con diversa información como: edad de los padres, educación de
los padres, número de visitas prenatales, peso al nacer, e información
acerca de la cantidad de cigarillos y bebida que consumia la madre
durante el embarazo. También se incluye información del feto.

Los nombres de las variables que contiene el dataset son:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(bwght2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "mage"    "meduc"   "monpre"  "npvis"   "fage"    "feduc"   "bwght"  
 [8] "omaps"   "fmaps"   "cigs"    "drink"   "lbw"     "vlbw"    "male"   
[15] "mwhte"   "mblck"   "moth"    "fwhte"   "fblck"   "foth"    "lbwght" 
[22] "magesq"  "npvissq"
\end{verbatim}

La variable dependiente que va ha ser explicada es \textbf{lbwght}, que
es el logaritmo de la variable \textbf{bwght} que es el peso al nacer.

Vamos a ajustar un modelo de mínimos cuadrados ordinarios (con todas las
variables explicativas) en el conjunto de entrenamiento e imprimir el
error de prueba obtenido.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{lm.fit =}\StringTok{ }\KeywordTok{lm}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos.train)}
\NormalTok{lm.pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit, }\DataTypeTok{newdata =}\NormalTok{ datos.test)}

\NormalTok{error.mco <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((datos.test[, }\StringTok{"lbwght"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{lm.pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.mco}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000518739
\end{verbatim}

Creamos una tabla para guardar los resultados de todos los modelos

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{3}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(results) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"None"}\NormalTok{,}\StringTok{"5 Cross Validation"}\NormalTok{,}\StringTok{"10 Cross Validation"}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(results) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"MCO"}\NormalTok{,}\StringTok{"MCO with Subset Selection"}\NormalTok{,}\StringTok{"MCO with Forward step wise"}\NormalTok{)}
\NormalTok{results <-}\StringTok{ }\KeywordTok{as.table}\NormalTok{(results)}

\NormalTok{results[}\StringTok{"MCO"}\NormalTok{, }\StringTok{"None"}\NormalTok{] =}\StringTok{ }\NormalTok{error.mco}
\end{Highlighting}
\end{Shaded}

Los datos del modelo y los coeficientes B los podemos ver a
continuación.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(lm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = lbwght ~ ., data = datos.train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51085 -0.00469  0.00483  0.01096  0.25896 

Coefficients: (2 not defined because of singularities)
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.033e+00  3.045e-02 230.942  < 2e-16 ***
mage        -1.245e-03  1.629e-03  -0.765   0.4446    
meduc       -4.244e-04  5.006e-04  -0.848   0.3967    
monpre       5.921e-04  7.333e-04   0.807   0.4196    
npvis        1.056e-04  6.817e-04   0.155   0.8769    
fage         2.315e-04  2.013e-04   1.150   0.2504    
feduc       -4.886e-06  4.527e-04  -0.011   0.9914    
bwght        2.979e-04  1.612e-06 184.811  < 2e-16 ***
omaps        9.290e-04  8.914e-04   1.042   0.2975    
fmaps        9.733e-03  2.103e-03   4.629 4.11e-06 ***
cigs         3.931e-04  2.164e-04   1.817   0.0696 .  
drink       -6.184e-04  3.206e-03  -0.193   0.8471    
lbw         -1.540e-01  9.160e-03 -16.816  < 2e-16 ***
vlbw        -3.316e-01  1.387e-02 -23.914  < 2e-16 ***
male        -7.498e-04  1.639e-03  -0.457   0.6475    
mwhte        3.539e-03  7.362e-03   0.481   0.6308    
mblck        2.381e-03  1.144e-02   0.208   0.8351    
moth                NA         NA      NA       NA    
fwhte       -5.610e-03  7.984e-03  -0.703   0.4824    
fblck        2.088e-03  1.173e-02   0.178   0.8588    
foth                NA         NA      NA       NA    
magesq       1.586e-05  2.710e-05   0.585   0.5586    
npvissq     -6.468e-06  2.138e-05  -0.302   0.7623    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.02708 on 1107 degrees of freedom
Multiple R-squared:  0.9804,    Adjusted R-squared:   0.98 
F-statistic:  2762 on 20 and 1107 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)          mage         meduc        monpre         npvis 
 7.032634e+00 -1.245338e-03 -4.244261e-04  5.920562e-04  1.056039e-04 
         fage         feduc         bwght         omaps         fmaps 
 2.315328e-04 -4.885967e-06  2.978545e-04  9.290155e-04  9.733477e-03 
         cigs         drink           lbw          vlbw          male 
 3.931007e-04 -6.183647e-04 -1.540387e-01 -3.316268e-01 -7.498069e-04 
        mwhte         mblck          moth         fwhte         fblck 
 3.538706e-03  2.381276e-03            NA -5.610199e-03  2.087637e-03 
         foth        magesq       npvissq 
           NA  1.585596e-05 -6.467952e-06 
\end{verbatim}

Podemos concluir que dos coeficientes fueron omitidos por estar las
respectivas variables muy correlacionadas(\textbf{moth} y
\textbf{foth}). También como se puede ver en el resultado de la
instrucción \textbf{summary} solo las variables:

\textbf{bwght} \textbf{fmaps}\\
\textbf{lbw}\\
\textbf{vlbw}

Son significativas para predecir \textbf{lbwght}.

\paragraph{b)}\label{b}

Procedemos a eliminar del Dataset las dos variables correlacionadas
\textbf{moth} y \textbf{foth}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos[, }\StringTok{"moth"}\NormalTok{] <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{datos[, }\StringTok{"foth"}\NormalTok{] <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{datos.train[, }\StringTok{"moth"}\NormalTok{] <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{datos.train[, }\StringTok{"foth"}\NormalTok{] <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{datos.test[, }\StringTok{"moth"}\NormalTok{] <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{datos.test[, }\StringTok{"foth"}\NormalTok{] <-}\StringTok{ }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

Luego ajustamos un MCO utilizando la Mejor Selección de Conjuntos

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(leaps)}

\NormalTok{nvariables <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{dim}\NormalTok{(datos)[}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{regfit.full =}\StringTok{ }\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos[train,], }\DataTypeTok{nvmax=}\NormalTok{nvariables)}

\NormalTok{predict.regsubsets=}\ControlFlowTok{function}\NormalTok{(object, newdata, id, ...) \{}
\NormalTok{  form=}\KeywordTok{as.formula}\NormalTok{(object}\OperatorTok{$}\NormalTok{call[[}\DecValTok{2}\NormalTok{]])}
\NormalTok{  mat=}\KeywordTok{model.matrix}\NormalTok{(form,newdata)}
\NormalTok{  coefi=}\KeywordTok{coef}\NormalTok{(object,}\DataTypeTok{id=}\NormalTok{id) }
\NormalTok{  xvars=}\KeywordTok{names}\NormalTok{(coefi)}
\NormalTok{  mat[,xvars]}\OperatorTok{%*%}\NormalTok{coefi}
\NormalTok{\}}

\NormalTok{k =}\StringTok{ }\DecValTok{10}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{folds=}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{k,}\KeywordTok{nrow}\NormalTok{(datos.train),}\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{table}\NormalTok{(folds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
folds
  1   2   3   4   5   6   7   8   9  10 
 97 100 109 129 107 103 120 120 112 131 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.errors=}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{,k,nvariables, }\DataTypeTok{dimnames =}\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{ , }\KeywordTok{paste}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)))}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)\{}
\NormalTok{  best.fit=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train[folds}\OperatorTok{!=}\NormalTok{j,],}
                      \DataTypeTok{nvmax=}\NormalTok{nvariables)}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)\{}
\NormalTok{    pred=}\KeywordTok{predict.regsubsets}\NormalTok{(best.fit,datos.train[folds}\OperatorTok{==}\NormalTok{j,],}\DataTypeTok{id=}\NormalTok{i)}
\NormalTok{    cv.errors[j,i]=}\KeywordTok{mean}\NormalTok{( (datos.train}\OperatorTok{$}\NormalTok{lbwght[folds}\OperatorTok{==}\NormalTok{j]}\OperatorTok{-}\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{rmse.cv=}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{apply}\NormalTok{(cv.errors,}\DecValTok{2}\NormalTok{,mean))}
\NormalTok{rmse.cv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         1          2          3          4          5          6 
0.04821566 0.03330704 0.03041755 0.03007367 0.03013112 0.03014558 
         7          8          9         10         11         12 
0.03015778 0.03015774 0.03027918 0.03029074 0.03025820 0.03022182 
        13         14         15         16         17         18 
0.03020467 0.03019556 0.03019658 0.03021205 0.03022838 0.03022756 
        19         20 
0.03022169 0.03022142 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.min}\NormalTok{(rmse.cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4 
4 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rmse.cv,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.best=}\KeywordTok{regsubsets}\NormalTok{ (lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train , }\DataTypeTok{nvmax=}\NormalTok{nvariables)}
\KeywordTok{coef}\NormalTok{(reg.best ,}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)         bwght         fmaps           lbw          vlbw 
 7.0123631237  0.0002976461  0.0105802459 -0.1543394286 -0.3309693951 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Modelo final acorde a las la mejor selección de conjuntos}
\NormalTok{fit.final <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lbwght }\OperatorTok{~}\StringTok{ }\NormalTok{bwght }\OperatorTok{+}\StringTok{ }\NormalTok{fmaps }\OperatorTok{+}\StringTok{ }\NormalTok{lbw }\OperatorTok{+}\StringTok{ }\NormalTok{vlbw, }\DataTypeTok{data =}\NormalTok{ datos.train)}
\KeywordTok{summary}\NormalTok{(fit.final)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51461 -0.00406  0.00584  0.01070  0.26192 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.012e+00  1.703e-02 411.683  < 2e-16 ***
bwght        2.976e-04  1.585e-06 187.769  < 2e-16 ***
fmaps        1.058e-02  1.834e-03   5.768 1.04e-08 ***
lbw         -1.543e-01  9.002e-03 -17.145  < 2e-16 ***
vlbw        -3.310e-01  1.373e-02 -24.109  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.02708 on 1123 degrees of freedom
Multiple R-squared:  0.9801,    Adjusted R-squared:   0.98 
F-statistic: 1.381e+04 on 4 and 1123 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.full=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{ datos[train,],}\DataTypeTok{nvmax=}\NormalTok{nvariables)}

\NormalTok{lm.pred =}\StringTok{ }\KeywordTok{predict.regsubsets}\NormalTok{(regfit.full, }\DataTypeTok{newdata =}\NormalTok{ datos.test, }\DataTypeTok{id=}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\NormalTok{error.mss <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((datos.test[, }\StringTok{"lbwght"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{lm.pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.mss }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000511914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{"MCO with Subset Selection"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.mss}
\end{Highlighting}
\end{Shaded}

El error de prueba obtenido es mas pequeño que con el MCO ordinario.

\paragraph{c)}\label{c}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nvariables <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{dim}\NormalTok{(datos)[}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{regfit.full =}\StringTok{ }\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos[train,], }\DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}

\NormalTok{k =}\StringTok{ }\DecValTok{10}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{folds=}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{k,}\KeywordTok{nrow}\NormalTok{(datos.train),}\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}

\KeywordTok{table}\NormalTok{(folds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
folds
  1   2   3   4   5   6   7   8   9  10 
 97 100 109 129 107 103 120 120 112 131 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.errors=}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{,k,nvariables, }\DataTypeTok{dimnames =}\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{ , }\KeywordTok{paste}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)))}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)\{}
\NormalTok{  best.fit=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train[folds}\OperatorTok{!=}\NormalTok{j,],}
                      \DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)\{}
\NormalTok{    pred=}\KeywordTok{predict.regsubsets}\NormalTok{(best.fit,datos.train[folds}\OperatorTok{==}\NormalTok{j,],}\DataTypeTok{id=}\NormalTok{i)}
\NormalTok{    cv.errors[j,i]=}\KeywordTok{mean}\NormalTok{( (datos.train}\OperatorTok{$}\NormalTok{lbwght[folds}\OperatorTok{==}\NormalTok{j]}\OperatorTok{-}\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{rmse.cv=}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{apply}\NormalTok{(cv.errors,}\DecValTok{2}\NormalTok{,mean))}
\NormalTok{rmse.cv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         1          2          3          4          5          6 
0.04821566 0.03330704 0.03041755 0.03007367 0.03013112 0.03015472 
         7          8          9         10         11         12 
0.03015426 0.03015431 0.03022841 0.03023027 0.03022682 0.03018473 
        13         14         15         16         17         18 
0.03019260 0.03018360 0.03018362 0.03021553 0.03022206 0.03022344 
        19         20 
0.03022860 0.03022142 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.min}\NormalTok{(rmse.cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4 
4 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rmse.cv,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.best=}\KeywordTok{regsubsets}\NormalTok{ (lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train , }\DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}
\KeywordTok{coef}\NormalTok{(reg.best ,}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)         bwght         fmaps           lbw          vlbw 
 7.0123631237  0.0002976461  0.0105802459 -0.1543394286 -0.3309693951 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Modelo final acorde a las la mejor selección de conjuntos}
\NormalTok{fit.final <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lbwght }\OperatorTok{~}\StringTok{ }\NormalTok{bwght }\OperatorTok{+}\StringTok{ }\NormalTok{fmaps }\OperatorTok{+}\StringTok{ }\NormalTok{lbw }\OperatorTok{+}\StringTok{ }\NormalTok{vlbw, }\DataTypeTok{data =}\NormalTok{ datos.train)}
\KeywordTok{summary}\NormalTok{(fit.final)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51461 -0.00406  0.00584  0.01070  0.26192 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.012e+00  1.703e-02 411.683  < 2e-16 ***
bwght        2.976e-04  1.585e-06 187.769  < 2e-16 ***
fmaps        1.058e-02  1.834e-03   5.768 1.04e-08 ***
lbw         -1.543e-01  9.002e-03 -17.145  < 2e-16 ***
vlbw        -3.310e-01  1.373e-02 -24.109  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.02708 on 1123 degrees of freedom
Multiple R-squared:  0.9801,    Adjusted R-squared:   0.98 
F-statistic: 1.381e+04 on 4 and 1123 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.full=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{ datos[train,],}\DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}

\NormalTok{lm.pred =}\StringTok{ }\KeywordTok{predict.regsubsets}\NormalTok{(regfit.full, }\DataTypeTok{newdata =}\NormalTok{ datos.test, }\DataTypeTok{id=}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\NormalTok{error.mss <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((datos.test[, }\StringTok{"lbwght"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{lm.pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.mss }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000511914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{"MCO with Forward step wise"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.mss}
\end{Highlighting}
\end{Shaded}

\paragraph{d)}\label{d}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nvariables <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{dim}\NormalTok{(datos)[}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{regfit.full =}\StringTok{ }\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos[train,], }\DataTypeTok{nvmax=}\NormalTok{nvariables)}

\NormalTok{predict.regsubsets=}\ControlFlowTok{function}\NormalTok{(object, newdata, id, ...) \{}
\NormalTok{  form=}\KeywordTok{as.formula}\NormalTok{(object}\OperatorTok{$}\NormalTok{call[[}\DecValTok{2}\NormalTok{]])}
\NormalTok{  mat=}\KeywordTok{model.matrix}\NormalTok{(form,newdata)}
\NormalTok{  coefi=}\KeywordTok{coef}\NormalTok{(object,}\DataTypeTok{id=}\NormalTok{id) }
\NormalTok{  xvars=}\KeywordTok{names}\NormalTok{(coefi)}
\NormalTok{  mat[,xvars]}\OperatorTok{%*%}\NormalTok{coefi}
\NormalTok{\}}

\NormalTok{k =}\StringTok{ }\DecValTok{5}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{folds=}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{k,}\KeywordTok{nrow}\NormalTok{(datos.train),}\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}

\KeywordTok{table}\NormalTok{(folds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
folds
  1   2   3   4   5 
197 238 210 240 243 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.errors=}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{,k,nvariables, }\DataTypeTok{dimnames =}\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{ , }\KeywordTok{paste}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)))}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)\{}
\NormalTok{  best.fit=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train[folds}\OperatorTok{!=}\NormalTok{j,],}
                      \DataTypeTok{nvmax=}\NormalTok{nvariables)}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)\{}
\NormalTok{    pred=}\KeywordTok{predict.regsubsets}\NormalTok{(best.fit,datos.train[folds}\OperatorTok{==}\NormalTok{j,],}\DataTypeTok{id=}\NormalTok{i)}
\NormalTok{    cv.errors[j,i]=}\KeywordTok{mean}\NormalTok{( (datos.train}\OperatorTok{$}\NormalTok{lbwght[folds}\OperatorTok{==}\NormalTok{j]}\OperatorTok{-}\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{rmse.cv=}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{apply}\NormalTok{(cv.errors,}\DecValTok{2}\NormalTok{,mean))}
\NormalTok{rmse.cv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         1          2          3          4          5          6 
0.04724912 0.03348780 0.03056028 0.03030288 0.03039083 0.03039898 
         7          8          9         10         11         12 
0.03048605 0.03045558 0.03052369 0.03053467 0.03051360 0.03051497 
        13         14         15         16         17         18 
0.03049662 0.03048933 0.03050103 0.03051083 0.03050089 0.03050719 
        19         20 
0.03050598 0.03050462 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.min}\NormalTok{(rmse.cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4 
4 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rmse.cv,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-13-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.best=}\KeywordTok{regsubsets}\NormalTok{ (lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train , }\DataTypeTok{nvmax=}\NormalTok{nvariables)}
\KeywordTok{coef}\NormalTok{(reg.best ,}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)         bwght         fmaps           lbw          vlbw 
 7.0123631237  0.0002976461  0.0105802459 -0.1543394286 -0.3309693951 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Modelo final acorde a las la mejor selección de conjuntos}
\NormalTok{fit.final <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lbwght }\OperatorTok{~}\StringTok{ }\NormalTok{bwght }\OperatorTok{+}\StringTok{ }\NormalTok{fmaps }\OperatorTok{+}\StringTok{ }\NormalTok{lbw }\OperatorTok{+}\StringTok{ }\NormalTok{vlbw, }\DataTypeTok{data =}\NormalTok{ datos.train)}
\KeywordTok{summary}\NormalTok{(fit.final)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51461 -0.00406  0.00584  0.01070  0.26192 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.012e+00  1.703e-02 411.683  < 2e-16 ***
bwght        2.976e-04  1.585e-06 187.769  < 2e-16 ***
fmaps        1.058e-02  1.834e-03   5.768 1.04e-08 ***
lbw         -1.543e-01  9.002e-03 -17.145  < 2e-16 ***
vlbw        -3.310e-01  1.373e-02 -24.109  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.02708 on 1123 degrees of freedom
Multiple R-squared:  0.9801,    Adjusted R-squared:   0.98 
F-statistic: 1.381e+04 on 4 and 1123 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.full=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{ datos[train,],}\DataTypeTok{nvmax=}\NormalTok{nvariables)}

\NormalTok{lm.pred =}\StringTok{ }\KeywordTok{predict.regsubsets}\NormalTok{(regfit.full, }\DataTypeTok{newdata =}\NormalTok{ datos.test, }\DataTypeTok{id=}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\NormalTok{error.mss <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((datos.test[, }\StringTok{"lbwght"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{lm.pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.mss }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000511914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{"MCO with Subset Selection"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.mss}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nvariables <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{dim}\NormalTok{(datos)[}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{regfit.full =}\StringTok{ }\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos[train,], }\DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}

\NormalTok{k =}\StringTok{ }\DecValTok{5}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{folds=}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{k,}\KeywordTok{nrow}\NormalTok{(datos.train),}\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}

\KeywordTok{table}\NormalTok{(folds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
folds
  1   2   3   4   5 
197 238 210 240 243 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.errors=}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{,k,nvariables, }\DataTypeTok{dimnames =}\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{ , }\KeywordTok{paste}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)))}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)\{}
\NormalTok{  best.fit=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train[folds}\OperatorTok{!=}\NormalTok{j,],}
                      \DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nvariables)\{}
\NormalTok{    pred=}\KeywordTok{predict.regsubsets}\NormalTok{(best.fit,datos.train[folds}\OperatorTok{==}\NormalTok{j,],}\DataTypeTok{id=}\NormalTok{i)}
\NormalTok{    cv.errors[j,i]=}\KeywordTok{mean}\NormalTok{( (datos.train}\OperatorTok{$}\NormalTok{lbwght[folds}\OperatorTok{==}\NormalTok{j]}\OperatorTok{-}\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{rmse.cv=}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{apply}\NormalTok{(cv.errors,}\DecValTok{2}\NormalTok{,mean))}
\NormalTok{rmse.cv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         1          2          3          4          5          6 
0.04724912 0.03348780 0.03056028 0.03030288 0.03039083 0.03039898 
         7          8          9         10         11         12 
0.03052785 0.03050632 0.03047763 0.03051874 0.03049464 0.03050382 
        13         14         15         16         17         18 
0.03047868 0.03047105 0.03047792 0.03048737 0.03048974 0.03048264 
        19         20 
0.03050674 0.03050462 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.min}\NormalTok{(rmse.cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4 
4 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rmse.cv,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-14-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.best=}\KeywordTok{regsubsets}\NormalTok{ (lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{datos.train , }\DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}
\KeywordTok{coef}\NormalTok{(reg.best ,}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)         bwght         fmaps           lbw          vlbw 
 7.0123631237  0.0002976461  0.0105802459 -0.1543394286 -0.3309693951 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Modelo final acorde a las la mejor selección de conjuntos}
\NormalTok{fit.final <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lbwght }\OperatorTok{~}\StringTok{ }\NormalTok{bwght }\OperatorTok{+}\StringTok{ }\NormalTok{fmaps }\OperatorTok{+}\StringTok{ }\NormalTok{lbw }\OperatorTok{+}\StringTok{ }\NormalTok{vlbw, }\DataTypeTok{data =}\NormalTok{ datos.train)}
\KeywordTok{summary}\NormalTok{(fit.final)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51461 -0.00406  0.00584  0.01070  0.26192 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.012e+00  1.703e-02 411.683  < 2e-16 ***
bwght        2.976e-04  1.585e-06 187.769  < 2e-16 ***
fmaps        1.058e-02  1.834e-03   5.768 1.04e-08 ***
lbw         -1.543e-01  9.002e-03 -17.145  < 2e-16 ***
vlbw        -3.310e-01  1.373e-02 -24.109  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.02708 on 1123 degrees of freedom
Multiple R-squared:  0.9801,    Adjusted R-squared:   0.98 
F-statistic: 1.381e+04 on 4 and 1123 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.full=}\KeywordTok{regsubsets}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{ datos[train,],}\DataTypeTok{nvmax=}\NormalTok{nvariables, }\DataTypeTok{method =} \StringTok{"forward"}\NormalTok{)}

\NormalTok{lm.pred =}\StringTok{ }\KeywordTok{predict.regsubsets}\NormalTok{(regfit.full, }\DataTypeTok{newdata =}\NormalTok{ datos.test, }\DataTypeTok{id=}\KeywordTok{which.min}\NormalTok{(rmse.cv))}
\NormalTok{error.mss <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((datos.test[, }\StringTok{"lbwght"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{lm.pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.mss }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000511914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{"MCO with Forward step wise"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.mss}
\end{Highlighting}
\end{Shaded}

\paragraph{e)}\label{e}

A continuación se muestran los modelos utilizados anteriormente con el
tipo de validación cruzada y su error de prueba correspondiente.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kable}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrr@{}}
\toprule
& None & 5 Cross Validation & 10 Cross Validation\tabularnewline
\midrule
\endhead
MCO & 0.0005187 & NA & NA\tabularnewline
MCO with Subset Selection & NA & 0.0005119 & 0.0005119\tabularnewline
MCO with Forward step wise & NA & 0.0005119 & 0.0005119\tabularnewline
Como se puede observar en la & tabla no hay & mucha diferencia ent & re
los errores de prueba obtenidos de estos enfoques.\tabularnewline
\bottomrule
\end{longtable}

\paragraph{f)}\label{f}

Para seleccionar el modelo que minimiza el error, como no hay mucha
diferencia entre los errores anteriores, escogimos el último modelo
analizado \textbf{MCO with Forward step wise}. En el análisis de este
modelo se determinaron las variables significativas al 5\%, son las que
se usan a continuación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.final <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lbwght }\OperatorTok{~}\StringTok{ }\NormalTok{bwght }\OperatorTok{+}\StringTok{ }\NormalTok{fmaps }\OperatorTok{+}\StringTok{ }\NormalTok{lbw }\OperatorTok{+}\StringTok{ }\NormalTok{vlbw, }\DataTypeTok{data =}\NormalTok{ datos.train)}

\NormalTok{lm.pred <-}\StringTok{ }\KeywordTok{predict.regsubsets}\NormalTok{(fit.final, }\DataTypeTok{newdata =}\NormalTok{ datos.test)}
\NormalTok{error.new =}\StringTok{  }\KeywordTok{mean}\NormalTok{((datos.test[, }\StringTok{"lbwght"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{lm.pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.new}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000511914
\end{verbatim}

El modelo con sólo cuatro variables explicativas con un nivel de
significación del 5\% (\textbf{bwght, fmaps, lbw, vlbw}) y un error de
5.1873897\times 10\^{}\{-4\}, es un poco mas preciso respecto al error
del MCO: 5.1873897\times 10\^{}\{-4\}.

Los \textbf{p-valores} se muestran a continuación:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit.final)[}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$coefficients
                 Estimate   Std. Error    t value      Pr(>|t|)
(Intercept)  7.0123631237 1.703342e-02 411.682667  0.000000e+00
bwght        0.0002976461 1.585174e-06 187.768796  0.000000e+00
fmaps        0.0105802459 1.834398e-03   5.767694  1.038118e-08
lbw         -0.1543394286 9.001872e-03 -17.145259  1.041566e-58
vlbw        -0.3309693951 1.372829e-02 -24.108564 7.864423e-104
\end{verbatim}

\paragraph{g)}\label{g}

Regresión de Ridge

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\KeywordTok{library}\NormalTok{(glmnet)}
\NormalTok{x=}\KeywordTok{model.matrix}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,datos)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{y=datos}\OperatorTok{$}\NormalTok{lbwght}
\NormalTok{y.test=y[test]}
\NormalTok{grid=}\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{length =}\DecValTok{100}\NormalTok{)}
\NormalTok{cv.ridge=}\KeywordTok{cv.glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{grid)}
\KeywordTok{plot}\NormalTok{(cv.ridge)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mejorlambda=cv.ridge}\OperatorTok{$}\NormalTok{lambda.min}
\NormalTok{mejorlambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.mod=}\KeywordTok{glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{grid)}
\NormalTok{ridge.pred=}\KeywordTok{predict}\NormalTok{(ridge.mod,}\DataTypeTok{s=}\NormalTok{mejorlambda ,}\DataTypeTok{newx=}\NormalTok{x[test ,])}

\NormalTok{error.ridge <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((ridge.pred}\OperatorTok{-}\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{] )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.ridge}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005855056
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"RIDGE"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.ridge}

\CommentTok{# Regla del "codo" de una DT del error de VC:}
\NormalTok{lambda.codo <-}\StringTok{ }\NormalTok{cv.ridge}\OperatorTok{$}\NormalTok{lambda.1se}
\NormalTok{lambda.codo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0231013
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.pred.}\DecValTok{2}\NormalTok{=}\KeywordTok{predict}\NormalTok{(ridge.mod,}\DataTypeTok{s=}\NormalTok{lambda.codo,}\DataTypeTok{newx=}\NormalTok{x[test ,])}
\NormalTok{error.ridge.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((ridge.pred.}\DecValTok{2}\OperatorTok{-}\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{] )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.ridge.}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000850785
\end{verbatim}

\paragraph{h)}\label{h}

Modelo de LASSO

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{cv.lasso=}\KeywordTok{cv.glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid)}
\KeywordTok{plot}\NormalTok{(cv.lasso)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-20-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestlam=cv.lasso}\OperatorTok{$}\NormalTok{lambda.min}
\NormalTok{bestlam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.mod=}\KeywordTok{glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{grid)}
\KeywordTok{plot}\NormalTok{(lasso.mod)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-20-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.pred=}\KeywordTok{predict}\NormalTok{(lasso.mod,}\DataTypeTok{s=}\NormalTok{bestlam,}\DataTypeTok{newx=}\NormalTok{x[test ,])}

\NormalTok{error.lasso <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((lasso.pred}\OperatorTok{-}\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{] )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.lasso}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000435723
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"LASSO"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.lasso}

\CommentTok{# Regla del "codo" de una DT del error de VC:}
\NormalTok{lambda.codo.l <-}\StringTok{ }\NormalTok{cv.lasso}\OperatorTok{$}\NormalTok{lambda.1se}
\NormalTok{lambda.codo.l}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01747528
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.pred.}\DecValTok{2}\NormalTok{=}\KeywordTok{predict}\NormalTok{(lasso.mod,}\DataTypeTok{s=}\NormalTok{lambda.codo.l,}\DataTypeTok{newx=}\NormalTok{x[test ,])}
\NormalTok{error.lasso.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((lasso.pred.}\DecValTok{2}\OperatorTok{-}\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{] )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.lasso.}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005318692
\end{verbatim}

Para obtener los coeficientes del modelo para el mejor lambda usamos la
función coef

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c =}\StringTok{ }\KeywordTok{coef}\NormalTok{(lasso.mod, }\DataTypeTok{s =}\NormalTok{ bestlam)}
\NormalTok{c}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21 x 1 sparse Matrix of class "dgCMatrix"
                        1
(Intercept)  7.1446826417
mage         .           
meduc        .           
monpre       .           
npvis        .           
fage         .           
feduc        .           
bwght        0.0002865851
omaps        .           
fmaps        .           
cigs         .           
drink        .           
lbw         -0.1261450669
vlbw        -0.2789833877
male         .           
mwhte        .           
mblck        .           
fwhte        .           
fblck        .           
magesq       .           
npvissq      .           
\end{verbatim}

Los coeficientes diferentes de 0 son 3.

\paragraph{i)}\label{i}

Regresión de Ridge con CV 5 veces

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\KeywordTok{library}\NormalTok{(glmnet)}
\NormalTok{x=}\KeywordTok{model.matrix}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{.,datos)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{y=datos}\OperatorTok{$}\NormalTok{lbwght}
\NormalTok{y.test=y[test]}
\NormalTok{grid=}\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{length =}\DecValTok{100}\NormalTok{)}
\NormalTok{cv.ridge=}\KeywordTok{cv.glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{grid, }\DataTypeTok{nfolds =} \DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cv.ridge)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-22-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mejorlambda=cv.ridge}\OperatorTok{$}\NormalTok{lambda.min}
\NormalTok{mejorlambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.mod=}\KeywordTok{glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{grid)}
\NormalTok{ridge.pred=}\KeywordTok{predict}\NormalTok{(ridge.mod,}\DataTypeTok{s=}\NormalTok{mejorlambda ,}\DataTypeTok{newx=}\NormalTok{x[test ,])}

\NormalTok{error.ridge <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((ridge.pred}\OperatorTok{-}\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{] )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.ridge}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005855056
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"RIDGE"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.ridge}
\end{Highlighting}
\end{Shaded}

Modelo de LASSO

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{cv.lasso=}\KeywordTok{cv.glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid, }\DataTypeTok{nfolds =} \DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cv.lasso)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-23-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestlam=cv.lasso}\OperatorTok{$}\NormalTok{lambda.min}
\NormalTok{bestlam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.mod=}\KeywordTok{glmnet}\NormalTok{(x[train ,],y[train],}\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{grid)}
\KeywordTok{plot}\NormalTok{(lasso.mod)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-23-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.pred=}\KeywordTok{predict}\NormalTok{(lasso.mod,}\DataTypeTok{s=}\NormalTok{bestlam,}\DataTypeTok{newx=}\NormalTok{x[test ,])}

\NormalTok{error.lasso <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((lasso.pred}\OperatorTok{-}\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{] )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.lasso}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000435723
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"LASSO"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.lasso}
\end{Highlighting}
\end{Shaded}

Para obtener los coeficientes del modelo para el mejor lambda usamos la
función coef

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c =}\StringTok{ }\KeywordTok{coef}\NormalTok{(lasso.mod, }\DataTypeTok{s =}\NormalTok{ bestlam)}
\NormalTok{c}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21 x 1 sparse Matrix of class "dgCMatrix"
                        1
(Intercept)  7.1446826417
mage         .           
meduc        .           
monpre       .           
npvis        .           
fage         .           
feduc        .           
bwght        0.0002865851
omaps        .           
fmaps        .           
cigs         .           
drink        .           
lbw         -0.1261450669
vlbw        -0.2789833877
male         .           
mwhte        .           
mblck        .           
fwhte        .           
fblck        .           
magesq       .           
npvissq      .           
\end{verbatim}

Los coeficientes diferentes de 0 son 3.

\paragraph{j)}\label{j}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pls)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{pcr.fit=}\KeywordTok{pcr}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos,}\DataTypeTok{subset=}\NormalTok{train,}\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation=}\StringTok{"CV"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(pcr.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: svdpc
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915   0.1892   0.1875   0.1420   0.1352   0.1354   0.1227
adjCV       0.1915   0.1892   0.1875   0.1415   0.1350   0.1353   0.1210
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV      0.1179   0.1091   0.1080    0.1078   0.04523   0.03479   0.03552
adjCV   0.1160   0.1087   0.1078    0.1081   0.04464   0.03412   0.03515
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03409   0.03372   0.03071   0.03073   0.03076   0.03077
adjCV   0.03396   0.03368   0.03050   0.03052   0.03055   0.03056
       20 comps
CV      0.03085
adjCV   0.03064

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
X        17.186   32.312    44.07    53.67    60.79    66.51    71.64
lbwght    3.228    6.003    47.20    52.10    52.19    66.74    69.55
        8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
X         76.75    80.83     84.77     88.28     90.92     93.30     95.43
lbwght    70.31    70.49     70.52     94.99     97.18     97.18     97.45
        15 comps  16 comps  17 comps  18 comps  19 comps  20 comps
X          97.30     98.86     99.53     99.80     99.97    100.00
lbwght     97.52     98.03     98.03     98.03     98.03     98.04
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{validationplot}\NormalTok{(pcr.fit,}\DataTypeTok{val.type=}\StringTok{"MSEP"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Número de Componentes Principales"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-25-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcr.cv <-}\StringTok{ }\KeywordTok{crossval}\NormalTok{(pcr.fit, }\DataTypeTok{segments =} \DecValTok{10}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{RMSEP}\NormalTok{(pcr.cv), }\DataTypeTok{legendpos=}\StringTok{"topright"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-25-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pcr.cv, }\DataTypeTok{what =} \StringTok{"validation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: svdpc
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915   0.1896   0.1885   0.1428   0.1344   0.1344   0.1217
adjCV       0.1915   0.1897   0.1885   0.1420   0.1342   0.1343   0.1195
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV      0.1216   0.1076    0.107    0.1070   0.04754   0.03762   0.03783
adjCV   0.1179   0.1073    0.108    0.1085   0.04679   0.03684   0.03738
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03610   0.03595   0.03319   0.03320   0.03325   0.03325
adjCV   0.03582   0.03576   0.03284   0.03286   0.03291   0.03290
       20 comps
CV      0.03341
adjCV   0.03306
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Selecciona el número de componentes principales}
\NormalTok{## Regla del codo: 1 d.t.}
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t. <-}\StringTok{ }\KeywordTok{selectNcomp}\NormalTok{(pcr.fit, }\DataTypeTok{method =} \StringTok{"onesigma"}\NormalTok{, }\DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation =} \StringTok{"CV"}\NormalTok{,}
                            \DataTypeTok{segments =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-25-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcr.pred=}\KeywordTok{predict}\NormalTok{(pcr.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.)}
\NormalTok{error.pcr <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pcr.pred }\OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pcr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005175971
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"PCA"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.pcr}
\end{Highlighting}
\end{Shaded}

Usando el Metodo de Componentes Principales con una Validación Cruzada
10 veces obtenemos la cantidad de componentes M = 16 y un error de
prueba de 5.1759714\times 10\^{}\{-4\}.

El mismo método pero con la validación cruzada 5 veces seria:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{pcr.fit=}\KeywordTok{pcr}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos,}\DataTypeTok{subset=}\NormalTok{train,}\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation=}\StringTok{"CV"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(pcr.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: svdpc
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915   0.1892   0.1875   0.1420   0.1352   0.1354   0.1227
adjCV       0.1915   0.1892   0.1875   0.1415   0.1350   0.1353   0.1210
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV      0.1179   0.1091   0.1080    0.1078   0.04523   0.03479   0.03552
adjCV   0.1160   0.1087   0.1078    0.1081   0.04464   0.03412   0.03515
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03409   0.03372   0.03071   0.03073   0.03076   0.03077
adjCV   0.03396   0.03368   0.03050   0.03052   0.03055   0.03056
       20 comps
CV      0.03085
adjCV   0.03064

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
X        17.186   32.312    44.07    53.67    60.79    66.51    71.64
lbwght    3.228    6.003    47.20    52.10    52.19    66.74    69.55
        8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
X         76.75    80.83     84.77     88.28     90.92     93.30     95.43
lbwght    70.31    70.49     70.52     94.99     97.18     97.18     97.45
        15 comps  16 comps  17 comps  18 comps  19 comps  20 comps
X          97.30     98.86     99.53     99.80     99.97    100.00
lbwght     97.52     98.03     98.03     98.03     98.03     98.04
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{validationplot}\NormalTok{(pcr.fit,}\DataTypeTok{val.type=}\StringTok{"MSEP"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Número de Componentes Principales"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-26-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcr.cv <-}\StringTok{ }\KeywordTok{crossval}\NormalTok{(pcr.fit, }\DataTypeTok{segments =} \DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{RMSEP}\NormalTok{(pcr.cv), }\DataTypeTok{legendpos=}\StringTok{"topright"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-26-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pcr.cv, }\DataTypeTok{what =} \StringTok{"validation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: svdpc
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 5 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915   0.1890   0.1879   0.1443   0.1344   0.1345   0.1189
adjCV       0.1915   0.1891   0.1881   0.1425   0.1342   0.1343   0.1149
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV      0.1154   0.1082   0.1063    0.1073   0.04799   0.03897   0.03801
adjCV   0.1109   0.1077   0.1066    0.1089   0.04688   0.03697   0.03727
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03550   0.03569   0.03342   0.03342   0.03346   0.03346
adjCV   0.03494   0.03530   0.03270   0.03271   0.03274   0.03274
       20 comps
CV      0.03361
adjCV   0.03288
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Selecciona el número de componentes principales}
\NormalTok{## Regla del codo: 1 d.t.}
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t. <-}\StringTok{ }\KeywordTok{selectNcomp}\NormalTok{(pcr.fit, }\DataTypeTok{method =} \StringTok{"onesigma"}\NormalTok{, }\DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation =} \StringTok{"CV"}\NormalTok{,}
                            \DataTypeTok{segments =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-26-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcr.pred=}\KeywordTok{predict}\NormalTok{(pcr.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.)}
\NormalTok{error.pcr <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pcr.pred }\OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pcr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005175971
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"PCA"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.pcr}
\end{Highlighting}
\end{Shaded}

Usando el Metodo de Componentes Principales con una Validación Cruzada 5
veces obtenemos la cantidad de componentes M = 16 y un error de prueba
de 5.1759714\times 10\^{}\{-4\}.

\paragraph{k)}\label{k}

PLS con Validación cruzada 10 veces

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{pls.fit=}\KeywordTok{plsr}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos,}\DataTypeTok{subset=}\NormalTok{train,}\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation=}\StringTok{"CV"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(pls.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: kernelpls
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915  0.09115  0.05192  0.03511  0.03159  0.03096  0.03079
adjCV       0.1915  0.09089  0.05173  0.03493  0.03142  0.03076  0.03059
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV     0.03085  0.03072  0.03075   0.03075   0.03078   0.03080   0.03081
adjCV  0.03063  0.03052  0.03054   0.03054   0.03057   0.03058   0.03059
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03080   0.03080   0.03082   0.03085   0.03084   0.03085
adjCV   0.03059   0.03059   0.03061   0.03064   0.03063   0.03064
       20 comps
CV      0.03085
adjCV   0.03064

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
X         11.77    21.71    32.67    40.98    52.12    60.42    64.80
lbwght    78.83    93.25    97.25    97.85    97.97    98.01    98.03
        8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
X         68.80    73.23     78.72     80.34     82.46     85.54     86.74
lbwght    98.03    98.03     98.03     98.03     98.03     98.03     98.04
        15 comps  16 comps  17 comps  18 comps  19 comps  20 comps
X          90.90     93.06     94.53     97.35     98.96    100.00
lbwght     98.04     98.04     98.04     98.04     98.04     98.04
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{validationplot}\NormalTok{(pls.fit,}\DataTypeTok{val.type=}\StringTok{"MSEP"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Número de Componentes Principales"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-27-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.cv <-}\StringTok{ }\KeywordTok{crossval}\NormalTok{(pls.fit, }\DataTypeTok{segments =} \DecValTok{10}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{RMSEP}\NormalTok{(pls.cv), }\DataTypeTok{legendpos=}\StringTok{"topright"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-27-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pls.cv, }\DataTypeTok{what =} \StringTok{"validation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: kernelpls
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915  0.09097  0.05354  0.03803  0.03407  0.03345  0.03326
adjCV       0.1915  0.09077  0.05322  0.03766  0.03377  0.03312  0.03292
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV     0.03330  0.03322  0.03323   0.03324   0.03325   0.03330   0.03332
adjCV  0.03296  0.03288  0.03289   0.03290   0.03291   0.03295   0.03297
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03332   0.03335   0.03337   0.03341   0.03339   0.03340
adjCV   0.03297   0.03300   0.03302   0.03305   0.03304   0.03305
       20 comps
CV      0.03341
adjCV   0.03306
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Utilizamos 4 componentes por el Mínimo Error de VC}
\NormalTok{pls.pred=}\KeywordTok{predict}\NormalTok{(pls.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\DecValTok{4}\NormalTok{)}
\NormalTok{error.pls <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pls.pred }\OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pls}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005495759
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Selecciona el número de componentes principales}
\NormalTok{## Regla del codo: 1 d.t.}
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t. <-}\StringTok{ }\KeywordTok{selectNcomp}\NormalTok{(pls.fit, }\DataTypeTok{method =} \StringTok{"onesigma"}\NormalTok{, }\DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation =} \StringTok{"CV"}\NormalTok{,}
                            \DataTypeTok{segments =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-27-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.pred.}\DecValTok{2}\NormalTok{=}\KeywordTok{predict}\NormalTok{(pls.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.)}
\NormalTok{error.pls.codo <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pls.pred.}\DecValTok{2} \OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pls.codo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005495759
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"PLS"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.pls.codo}

\NormalTok{## Regla de la permutación: se selecciona el ncomp que nos da el min Error de VC}
\NormalTok{ncomp.perm <-}\StringTok{ }\KeywordTok{selectNcomp}\NormalTok{(pls.fit, }\DataTypeTok{method =} \StringTok{"randomization"}\NormalTok{, }\DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-27-4.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncomp.perm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.pred.}\DecValTok{3}\NormalTok{=}\KeywordTok{predict}\NormalTok{(pls.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\NormalTok{ncomp.perm)}
\NormalTok{error.pls.perm <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pls.pred.}\DecValTok{3} \OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pls.perm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005495759
\end{verbatim}

PLS con Validación cruzada 5 veces

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{pls.fit=}\KeywordTok{plsr}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{datos,}\DataTypeTok{subset=}\NormalTok{train,}\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation=}\StringTok{"CV"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(pls.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: kernelpls
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915  0.09115  0.05192  0.03511  0.03159  0.03096  0.03079
adjCV       0.1915  0.09089  0.05173  0.03493  0.03142  0.03076  0.03059
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV     0.03085  0.03072  0.03075   0.03075   0.03078   0.03080   0.03081
adjCV  0.03063  0.03052  0.03054   0.03054   0.03057   0.03058   0.03059
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03080   0.03080   0.03082   0.03085   0.03084   0.03085
adjCV   0.03059   0.03059   0.03061   0.03064   0.03063   0.03064
       20 comps
CV      0.03085
adjCV   0.03064

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
X         11.77    21.71    32.67    40.98    52.12    60.42    64.80
lbwght    78.83    93.25    97.25    97.85    97.97    98.01    98.03
        8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
X         68.80    73.23     78.72     80.34     82.46     85.54     86.74
lbwght    98.03    98.03     98.03     98.03     98.03     98.03     98.04
        15 comps  16 comps  17 comps  18 comps  19 comps  20 comps
X          90.90     93.06     94.53     97.35     98.96    100.00
lbwght     98.04     98.04     98.04     98.04     98.04     98.04
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{validationplot}\NormalTok{(pls.fit,}\DataTypeTok{val.type=}\StringTok{"MSEP"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Número de Componentes Principales"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-28-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.cv <-}\StringTok{ }\KeywordTok{crossval}\NormalTok{(pls.fit, }\DataTypeTok{segments =} \DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{RMSEP}\NormalTok{(pls.cv), }\DataTypeTok{legendpos=}\StringTok{"topright"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-28-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pls.cv, }\DataTypeTok{what =} \StringTok{"validation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 1128 20 
    Y dimension: 1128 1
Fit method: kernelpls
Number of components considered: 20

VALIDATION: RMSEP
Cross-validated using 5 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          0.1915  0.09076  0.05483  0.03861  0.03414  0.03360  0.03339
adjCV       0.1915  0.09023  0.05378  0.03777  0.03355  0.03292  0.03271
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV     0.03339  0.03345  0.03345   0.03344   0.03347   0.03351   0.03352
adjCV  0.03269  0.03273  0.03273   0.03273   0.03275   0.03279   0.03279
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV      0.03353   0.03356   0.03358   0.03361   0.03360   0.03361
adjCV   0.03281   0.03283   0.03285   0.03287   0.03287   0.03287
       20 comps
CV      0.03361
adjCV   0.03288
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Utilizamos 4 componentes por el Mínimo Error de VC}
\NormalTok{pls.pred=}\KeywordTok{predict}\NormalTok{(pls.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\DecValTok{4}\NormalTok{)}
\NormalTok{error.pls <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pls.pred }\OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pls}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005495759
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Selecciona el número de componentes principales}
\NormalTok{## Regla del codo: 1 d.t.}
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t. <-}\StringTok{ }\KeywordTok{selectNcomp}\NormalTok{(pls.fit, }\DataTypeTok{method =} \StringTok{"onesigma"}\NormalTok{, }\DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{validation =} \StringTok{"CV"}\NormalTok{,}
                            \DataTypeTok{segments =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-28-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.pred.}\DecValTok{2}\NormalTok{=}\KeywordTok{predict}\NormalTok{(pls.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\NormalTok{ncomp.}\FloatTok{1.}\NormalTok{d.t.)}
\NormalTok{error.pls.codo <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pls.pred.}\DecValTok{2} \OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pls.codo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005495759
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"PLS"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.pls.codo}

\NormalTok{## Regla de la permutación: se selecciona el ncomp que nos da el min Error de VC}
\NormalTok{ncomp.perm <-}\StringTok{ }\KeywordTok{selectNcomp}\NormalTok{(pls.fit, }\DataTypeTok{method =} \StringTok{"randomization"}\NormalTok{, }\DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-28-4.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncomp.perm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.pred.}\DecValTok{3}\NormalTok{=}\KeywordTok{predict}\NormalTok{(pls.fit,}\DataTypeTok{newdata=}\NormalTok{x[test,],}\DataTypeTok{ncomp=}\NormalTok{ncomp.perm)}
\NormalTok{error.pls.perm <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((pls.pred.}\DecValTok{3} \OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.pls.perm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0005495759
\end{verbatim}

\paragraph{l)}\label{l}

Modelo LASSO con la restricción de Red Elástica (LASSO with Elastic Net)
en el conjunto de entrenamiento, con el α y el λ elegido mediante la
Validación Cruzada 10-Veces.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{lambda.grid <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{alpha.grid <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.05}\NormalTok{)}

\NormalTok{Control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{busca.grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{alpha =}\NormalTok{ alpha.grid, }\DataTypeTok{lambda =}\NormalTok{ lambda.grid)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{mi.entrenamiento <-}\StringTok{ }\KeywordTok{train}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ datos.train, }\DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{, }
                          \DataTypeTok{tuneGrid =}\NormalTok{ busca.grid, }\DataTypeTok{trControl =}\NormalTok{ Control,}
                          \DataTypeTok{tuneLength =} \DecValTok{10}\NormalTok{,}
                          \DataTypeTok{standardize =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{maxit =} \DecValTok{1000000}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(mi.entrenamiento)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-29-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(mi.entrenamiento)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$names
 [1] "method"       "modelInfo"    "modelType"    "results"     
 [5] "pred"         "bestTune"     "call"         "dots"        
 [9] "metric"       "control"      "finalModel"   "preProcess"  
[13] "trainingData" "resample"     "resampledCM"  "perfNames"   
[17] "maximize"     "yLimits"      "times"        "levels"      
[21] "terms"        "coefnames"    "xlevels"     

$class
[1] "train"         "train.formula"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    alpha lambda
901  0.45   0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mi.modelo.glmnet <-}\StringTok{ }\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{finalModel}
\KeywordTok{coef}\NormalTok{(mi.modelo.glmnet, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21 x 1 sparse Matrix of class "dgCMatrix"
                        1
(Intercept)  7.1073653340
mage         .           
meduc        .           
monpre       .           
npvis        .           
fage         .           
feduc        .           
bwght        0.0002832945
omaps        .           
fmaps        0.0054557468
cigs         .           
drink        .           
lbw         -0.1559228772
vlbw        -0.3073639506
male         .           
mwhte        .           
mblck        .           
fwhte        .           
fblck        .           
magesq       .           
npvissq      .           
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mej.modelo <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x[train ,],y[train], }\DataTypeTok{alpha=}\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{alpha,}
                     \DataTypeTok{lambda =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda)}
\NormalTok{c =}\StringTok{ }\KeywordTok{coef}\NormalTok{(mej.modelo, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda)}
\KeywordTok{cbind}\NormalTok{(}\KeywordTok{coef}\NormalTok{(mej.modelo, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda), }\KeywordTok{coef}\NormalTok{(mi.modelo.glmnet, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21 x 2 sparse Matrix of class "dgCMatrix"
                        1             1
(Intercept)  7.1073855303  7.1073653340
mage         .             .           
meduc        .             .           
monpre       .             .           
npvis        .             .           
fage         .             .           
feduc        .             .           
bwght        0.0002832938  0.0002832945
omaps        .             .           
fmaps        0.0054537954  0.0054557468
cigs         .             .           
drink        .             .           
lbw         -0.1559405441 -0.1559228772
vlbw        -0.3073522220 -0.3073639506
male         .             .           
mwhte        .             .           
mblck        .             .           
fwhte        .             .           
fblck        .             .           
magesq       .             .           
npvissq      .             .           
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lre.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mej.modelo,}\DataTypeTok{s=}\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda,}\DataTypeTok{newx=}\NormalTok{x[test ,])}

\NormalTok{error.lassoelastic <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((lre.pred }\OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.lassoelastic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0004796094
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"LASSO with Elastic Net"}\NormalTok{, }\StringTok{"10 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.lassoelastic}
\end{Highlighting}
\end{Shaded}

El error de prueba obtenido es 4.7960944\times 10\^{}\{-4\} y la
cantidad de coeficientes distintos de 0 es 4.

\paragraph{m)}\label{m}

Modelo LASSO con la restricción de Red Elástica (LASSO with Elastic Net)
en el conjunto de entrenamiento, con el α y el λ elegido mediante la
Validación Cruzada 5-Veces.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{lambda.grid <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{alpha.grid <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{Control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{5}\NormalTok{)}
\NormalTok{busca.grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{alpha =}\NormalTok{ alpha.grid, }\DataTypeTok{lambda =}\NormalTok{ lambda.grid)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{mi.entrenamiento <-}\StringTok{ }\KeywordTok{train}\NormalTok{(lbwght}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ datos.train, }\DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{, }
                          \DataTypeTok{tuneGrid =}\NormalTok{ busca.grid, }\DataTypeTok{trControl =}\NormalTok{ Control,}
                          \DataTypeTok{tuneLength =} \DecValTok{5}\NormalTok{,}
                          \DataTypeTok{standardize =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{maxit =} \DecValTok{1000000}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(mi.entrenamiento)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(mi.entrenamiento)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$names
 [1] "method"       "modelInfo"    "modelType"    "results"     
 [5] "pred"         "bestTune"     "call"         "dots"        
 [9] "metric"       "control"      "finalModel"   "preProcess"  
[13] "trainingData" "resample"     "resampledCM"  "perfNames"   
[17] "maximize"     "yLimits"      "times"        "levels"      
[21] "terms"        "coefnames"    "xlevels"     

$class
[1] "train"         "train.formula"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    alpha lambda
701  0.35   0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mi.modelo.glmnet <-}\StringTok{ }\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{finalModel}
\KeywordTok{coef}\NormalTok{(mi.modelo.glmnet, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21 x 1 sparse Matrix of class "dgCMatrix"
                        1
(Intercept)  7.0950292590
mage         .           
meduc        .           
monpre       .           
npvis        .           
fage         .           
feduc        .           
bwght        0.0002826996
omaps        .           
fmaps        0.0070610993
cigs         .           
drink        .           
lbw         -0.1609479918
vlbw        -0.3115200778
male         .           
mwhte        .           
mblck        .           
fwhte        .           
fblck        .           
magesq       .           
npvissq      .           
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mej.modelo <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x[train ,],y[train], }\DataTypeTok{alpha=}\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{alpha,}
                     \DataTypeTok{lambda =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda)}
\NormalTok{c =}\StringTok{ }\KeywordTok{coef}\NormalTok{(mej.modelo, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda)}
\KeywordTok{cbind}\NormalTok{(}\KeywordTok{coef}\NormalTok{(mej.modelo, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda), }\KeywordTok{coef}\NormalTok{(mi.modelo.glmnet, }\DataTypeTok{s =}\NormalTok{ mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21 x 2 sparse Matrix of class "dgCMatrix"
                        1             1
(Intercept)  7.0950610984  7.0950292590
mage         .             .           
meduc        .             .           
monpre       .             .           
npvis        .             .           
fage         .             .           
feduc        .             .           
bwght        0.0002826972  0.0002826996
omaps        .             .           
fmaps        0.0070584853  0.0070610993
cigs         .             .           
drink        .             .           
lbw         -0.1609638819 -0.1609479918
vlbw        -0.3115141652 -0.3115200778
male         .             .           
mwhte        .             .           
mblck        .             .           
fwhte        .             .           
fblck        .             .           
magesq       .             .           
npvissq      .             .           
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lre.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mej.modelo,}\DataTypeTok{s=}\NormalTok{mi.entrenamiento}\OperatorTok{$}\NormalTok{bestTune}\OperatorTok{$}\NormalTok{lambda,}\DataTypeTok{newx=}\NormalTok{x[test ,])}

\NormalTok{error.lassoelastic <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((lre.pred }\OperatorTok{-}\StringTok{ }\NormalTok{datos.test[, }\StringTok{"lbwght"}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{error.lassoelastic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0004959331
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results2[}\StringTok{"LASSO with Elastic Net"}\NormalTok{, }\StringTok{"5 Cross Validation"}\NormalTok{] =}\StringTok{ }\NormalTok{error.lassoelastic}
\end{Highlighting}
\end{Shaded}

El error de prueba obtenido es 4.9593309\times 10\^{}\{-4\} y la
cantidad de coeficientes distintos de 0 es 4.

\paragraph{o)}\label{o}

Ajusta un modelo Rigorous LASSO (RLASSO) sobre el conjunto de
entrenamiento con el lambda elegido mediante la penalización dependiente
de los datos

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = FALSE, X.dependent.lambda = TRUE)

Post-Lasso Estimation:  FALSE 

Total number of variables: 20
Number of selected variables: 1 

Residuals: 
      Min        1Q    Median        3Q       Max 
-1.070893  0.002102  0.011650  0.015554  0.016828 

            Estimate
(Intercept)    7.154
bwght          0.000

Residual standard error: 0.05398
Multiple R-squared:  0.9205
Adjusted R-squared:  0.9204
Joint significance test:
 the sup score statistic for joint significance test is  3516 with a p-value of     0
\end{verbatim}

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = FALSE, X.dependent.lambda = TRUE)

Coefficients:
(Intercept)         mage        meduc       monpre        npvis  
  7.1543986    0.0000000    0.0000000    0.0000000    0.0000000  
       fage        feduc        bwght        omaps        fmaps  
  0.0000000    0.0000000    0.0002827    0.0000000    0.0000000  
       cigs        drink          lbw         vlbw         male  
  0.0000000    0.0000000    0.0000000    0.0000000    0.0000000  
      mwhte        mblck        fwhte        fblck       magesq  
  0.0000000    0.0000000    0.0000000    0.0000000    0.0000000  
    npvissq  
  0.0000000  
\end{verbatim}

Informa el error de prueba obtenido junto con el número de coeficientes
estimados diferentes de cero.

Total de variables = \textbf{20} Variables seleccionadas = \textbf{1}
p-value = \textbf{0}

\begin{verbatim}
[1] 0.0009358474
\end{verbatim}

El error de prueba de Rigorous Lasso con penalización dependiente de los
datos es 9.3584738\times 10\^{}\{-4\}

Con el lambda elegido mediante la penalización independiente de los
datos

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = FALSE)

Post-Lasso Estimation:  FALSE 

Total number of variables: 20
Number of selected variables: 1 

Residuals: 
      Min        1Q    Median        3Q       Max 
-1.070893  0.002102  0.011650  0.015554  0.016828 

            Estimate
(Intercept)    7.154
bwght          0.000

Residual standard error: 0.05398
Multiple R-squared:  0.9205
Adjusted R-squared:  0.9204
Joint significance test:
 the sup score statistic for joint significance test is  3516 with a p-value of     0
\end{verbatim}

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = FALSE)

Coefficients:
(Intercept)         mage        meduc       monpre        npvis  
  7.1543986    0.0000000    0.0000000    0.0000000    0.0000000  
       fage        feduc        bwght        omaps        fmaps  
  0.0000000    0.0000000    0.0002827    0.0000000    0.0000000  
       cigs        drink          lbw         vlbw         male  
  0.0000000    0.0000000    0.0000000    0.0000000    0.0000000  
      mwhte        mblck        fwhte        fblck       magesq  
  0.0000000    0.0000000    0.0000000    0.0000000    0.0000000  
    npvissq  
  0.0000000  
\end{verbatim}

Informa el error de prueba obtenido junto con el número de coeficientes
estimados diferentes de cero.

Total de variables = \textbf{20} Variables seleccionadas = \textbf{1}
p-value = \textbf{0}

\begin{verbatim}
[1] 0.0009358474
\end{verbatim}

El error de prueba de Rigorous Lasso con penalización independiente de
los datos es 9.3584738\times 10\^{}\{-4\}

Calcula el error de prueba de los dos modelos mediante el ajuste de
Post-LASSO.

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = TRUE)

Post-Lasso Estimation:  TRUE 

Total number of variables: 20
Number of selected variables: 2 

Residuals: 
      Min        1Q    Median        3Q       Max 
-0.726393 -0.003614  0.006465  0.011053  0.193903 

            Estimate
(Intercept)    7.094
bwght          0.000
lbw           -0.293

Residual standard error: 0.03454
Multiple R-squared:  0.9675
Adjusted R-squared:  0.9674
Joint significance test:
 the sup score statistic for joint significance test is  3516 with a p-value of     0
\end{verbatim}

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = TRUE)

Coefficients:
(Intercept)         mage        meduc       monpre        npvis  
  7.0936376    0.0000000    0.0000000    0.0000000    0.0000000  
       fage        feduc        bwght        omaps        fmaps  
  0.0000000    0.0000000    0.0003018    0.0000000    0.0000000  
       cigs        drink          lbw         vlbw         male  
  0.0000000    0.0000000   -0.2934100    0.0000000    0.0000000  
      mwhte        mblck        fwhte        fblck       magesq  
  0.0000000    0.0000000    0.0000000    0.0000000    0.0000000  
    npvissq  
  0.0000000  
\end{verbatim}

\begin{verbatim}

Call:
rlasso.default(x = x, y = y, post = TRUE)

(Intercept)        bwght          lbw  
  7.0936376    0.0003018   -0.2934100  
\end{verbatim}

Informa el error de prueba obtenido junto con el número de coeficientes
estimados diferentes de cero.

Total de variables = \textbf{20} Variables seleccionadas = \textbf{1}
p-value = \textbf{0}

\begin{verbatim}
[1] 0.0005005301
\end{verbatim}

El error de prueba Post-Lasso es 5.0053005\times 10\^{}\{-4\}

\paragraph{p)}\label{p}

Contrastar la significación individual de los coeficientes estimados del
modelo final. Utiliza el nivel de significación del 5\%.

\begin{verbatim}
[1] "Estimates and significance testing of the effect of target variables"
         Estimate. Std. Error t value Pr(>|t|)    
mage     3.843e-05  2.058e-03   0.019 0.985100    
meduc    2.686e-04  6.252e-04   0.430 0.667424    
monpre   1.284e-03  8.585e-04   1.495 0.134885    
npvis   -2.455e-04  8.602e-04  -0.285 0.775333    
fage     2.723e-04  2.544e-04   1.070 0.284482    
feduc   -8.788e-04  5.669e-04  -1.550 0.121109    
bwght    2.976e-04  1.583e-06 188.019  < 2e-16 ***
omaps   -9.043e-04  1.125e-03  -0.804 0.421432    
fmaps    1.023e-02  2.644e-03   3.871 0.000108 ***
cigs     4.469e-04  2.729e-04   1.638 0.101460    
drink    7.396e-04  4.023e-03   0.184 0.854123    
lbw     -1.380e-01  8.776e-03 -15.727  < 2e-16 ***
vlbw    -2.048e-01  1.159e-02 -17.672  < 2e-16 ***
male    -2.454e-03  2.057e-03  -1.193 0.232796    
mwhte    2.616e-03  9.354e-03   0.280 0.779745    
mblck   -5.379e-03  1.251e-02  -0.430 0.667106    
fwhte   -7.936e-03  1.010e-02  -0.786 0.431914    
fblck    6.145e-04  1.250e-02   0.049 0.960801    
magesq   6.233e-06  3.356e-05   0.186 0.852649    
npvissq -4.377e-05  2.683e-05  -1.632 0.102755    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\includegraphics{main_files/figure-latex/unnamed-chunk-37-1.pdf} Sólo
las variables *\textbf{bwght, fmaps, lbw y vlbw} son significativas
sobre un nivel de confianza del 1\%

Ahora eliminamos las variables no significativas

\begin{verbatim}
[1] "Estimates and significance testing of the effect of target variables"
       Estimate. Std. Error t value Pr(>|t|)    
bwght  2.976e-04  1.583e-06 188.019  < 2e-16 ***
fmaps  1.058e-02  1.832e-03   5.775 7.68e-09 ***
lbw   -1.541e-01  8.984e-03 -17.148  < 2e-16 ***
vlbw  -3.310e-01  1.448e-02 -22.861  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Las cuatro variables son muy significativas.

Informa el error de prueba obtenido

\begin{verbatim}
[1] 0.0007593627
\end{verbatim}

El error de prueba obtenido es 7.5936268\times 10\^{}\{-4\}

Basándose en contrastes de significación individual, selecciona modelos
que contengan sólo variables significativas al 5\% de significación y
estima el error de prueba de estos modelos (mediante el ajuste de
Post-LASSO).

\begin{verbatim}

Call:
rlasso.default(x = x.nuevo.1, y = y, post = TRUE)

Post-Lasso Estimation:  TRUE 

Total number of variables: 4
Number of selected variables: 4 

Residuals: 
      Min        1Q    Median        3Q       Max 
-0.514608 -0.004064  0.005844  0.010699  0.261923 

            Estimate
(Intercept)    7.012
bwght          0.000
fmaps          0.011
lbw           -0.154
vlbw          -0.331

Residual standard error: 0.02703
Multiple R-squared:  0.9801
Adjusted R-squared:   0.98
Joint significance test:
 the sup score statistic for joint significance test is  3516 with a p-value of     0
\end{verbatim}

\begin{verbatim}
[1] 0.000511914
\end{verbatim}

El error de prueba de estos modelos es 5.1191402\times 10\^{}\{-4\}

\paragraph{q)}\label{q}

A continuación se muestran los modelos utilizados anteriormente con el
tipo de validación cruzada y su error de prueba correspondiente.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kable}\NormalTok{(results2)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrr@{}}
\toprule
& None & 5 Cross Validation & 10 Cross Validation\tabularnewline
\midrule
\endhead
RIDGE & NA & 0.0005855 & 0.0005855\tabularnewline
LASSO & NA & 0.0004357 & 0.0004357\tabularnewline
PCA & NA & 0.0005176 & 0.0005176\tabularnewline
PLS & NA & 0.0005496 & 0.0005496\tabularnewline
LASSO with Elastic Net & NA & 0.0004959 & 0.0004796\tabularnewline
Rigorous LASSO Independent L & 0.0009358 & NA & NA\tabularnewline
Rigorous LASSO Dependent L & 0.0009358 & NA & NA\tabularnewline
Rigorous LASSO Post-Lasso & 0.0005005 & NA & NA\tabularnewline
\bottomrule
\end{longtable}

\paragraph{r)}\label{r}

Haz una tabla con los coeficientes estimados con sus p-valores del
modelo con el mejor ajuste. Comenta tus resultados. Interpreta los
coeficientes estimados del modelo con el mejor ajuste.

Vemos que las 2 últimas variables explicativas stock price (sp94) y log
(lsp90) son muy significativas sobre el 1\% de nivel de confianza

Por cada dólar \$ que aumenta el stock price (sp94), el rendimiento
(return) aumenta en un 1,252 \% Teniendo en cuenta la interpretación de
los coeficientes en una regresión linear \textless{}-- log, por cada
dólar \$ que aumenta log(stock price90), el rendimiento disminuye en un
0,7140 \%


\end{document}
