---
title: "Trabajo Final"
author: "Reinier Mujica"
date: "13 de enero de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, comment = NA)
```

### Inicialización
Limpiamos todas las variables del espacio de trabajo y el terminal de R.
```{r}
rm(list=ls())
cat("\014")
library(knitr)
```
Cargamos el paquete de wooldridge y los datos
```{r}
library(wooldridge)
attach(bwght2)
```
Fijamos la semilla 
```{r}
set.seed(100)
```


### Limpiar los datos
Vamos a remover los valores NA de los datos
```{r}
datos = na.omit(bwght2)
```

### División del conjunto de datos
```{r}
train.size = round(dim(datos)[1] * 0.7)
train = sample(1:dim(datos)[1], train.size)
test = -train
datos.train = datos[train, ]
datos.test = datos[test, ]
test.size = dim(datos.test)[1]
```


### Preguntas:

#### a) 
El conjunto de datos es bwght2 y contiene información sobre nacimientos. Estos datos han sido obtenidos de certificados de 
nacimiento y desfunción y también de información de natalidad y mortalidad del Centro Nacional de Estadística para la Salud. 
El dataset contiene varias variables con diversa información como: edad de los padres, educación de los padres, número de visitas prenatales, peso al nacer, e información acerca de la cantidad de cigarillos y bebida que consumia la madre durante el embarazo. También se incluye información del feto.

Los nombres de las variables que contiene el dataset son:
```{r}
names(bwght2)
```
La variable dependiente que va ha ser explicada es **lbwght**, que es el logaritmo de la variable **bwght** que es el peso al nacer.

Vamos a ajustar un modelo de mínimos cuadrados ordinarios (con todas las variables explicativas) en el conjunto
de entrenamiento e imprimir el error de prueba obtenido.
```{r}
set.seed(100)
lm.fit = lm(lbwght~., data=datos.train)
lm.pred = predict(lm.fit, newdata = datos.test)

error.mco <- mean((datos.test[, "lbwght"] - lm.pred)^2)
error.mco
```

Creamos una tabla para guardar los resultados de todos los modelos
```{r}
results <- matrix(NA, nrow = 3, ncol = 3)
colnames(results) <- c("None","5 Cross Validation","10 Cross Validation")
rownames(results) <- c("MCO","MCO with Subset Selection","MCO with Forward step wise")
results <- as.table(results)

results["MCO", "None"] = error.mco

```

Los datos del modelo y los coeficientes B los podemos ver a continuación.
```{r}
summary(lm.fit)
lm.fit$coefficients
```

Podemos concluir que dos coeficientes fueron omitidos por estar las respectivas variables muy correlacionadas(**moth** y **foth**). También como se puede ver en el resultado de la instrucción **summary** solo las variables:

**bwght**
**fmaps**    
**lbw**        
**vlbw**        

Son significativas para predecir **lbwght**.

#### b)
Procedemos a eliminar del Dataset las dos variables correlacionadas **moth** y **foth**
```{r}
datos[, "moth"] <- NULL
datos[, "foth"] <- NULL
datos.train[, "moth"] <- NULL
datos.train[, "foth"] <- NULL
datos.test[, "moth"] <- NULL
datos.test[, "foth"] <- NULL

```

Luego ajustamos un MCO utilizando la Mejor Selección de Conjuntos
```{r}
library(leaps)

nvariables <- as.numeric(dim(datos)[2] - 1)
regfit.full = regsubsets(lbwght~., data=datos[train,], nvmax=nvariables)

predict.regsubsets=function(object, newdata, id, ...) {
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id) 
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

k = 10
set.seed(100)
folds=sample(1:k,nrow(datos.train),replace=TRUE)
table(folds)
cv.errors=matrix(NA,k,nvariables, dimnames =list(NULL , paste(1:nvariables)))
for(j in 1:k){
  best.fit=regsubsets(lbwght~.,data=datos.train[folds!=j,],
                      nvmax=nvariables)
  for(i in 1:nvariables){
    pred=predict.regsubsets(best.fit,datos.train[folds==j,],id=i)
    cv.errors[j,i]=mean( (datos.train$lbwght[folds==j]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
rmse.cv
which.min(rmse.cv)
plot(rmse.cv,pch=19,type="b")
reg.best=regsubsets (lbwght~.,data=datos.train , nvmax=nvariables)
coef(reg.best ,which.min(rmse.cv))

# Modelo final acorde a las la mejor selección de conjuntos
fit.final <- lm(lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)
summary(fit.final)

regfit.full=regsubsets(lbwght~.,data= datos[train,],nvmax=nvariables)

lm.pred = predict.regsubsets(regfit.full, newdata = datos.test, id=which.min(rmse.cv))
error.mss <- mean((datos.test[, "lbwght"] - lm.pred)^2)
error.mss 

results["MCO with Subset Selection", "10 Cross Validation"] = error.mss
```
El error de prueba obtenido es mas pequeño que con el MCO ordinario.

#### c)
```{r}
nvariables <- as.numeric(dim(datos)[2] - 1)
regfit.full = regsubsets(lbwght~., data=datos[train,], nvmax=nvariables, method = "forward")

k = 10
set.seed(100)
folds=sample(1:k,nrow(datos.train),replace=TRUE)

table(folds)
cv.errors=matrix(NA,k,nvariables, dimnames =list(NULL , paste(1:nvariables)))
for(j in 1:k){
  best.fit=regsubsets(lbwght~.,data=datos.train[folds!=j,],
                      nvmax=nvariables, method = "forward")
  for(i in 1:nvariables){
    pred=predict.regsubsets(best.fit,datos.train[folds==j,],id=i)
    cv.errors[j,i]=mean( (datos.train$lbwght[folds==j]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
rmse.cv
which.min(rmse.cv)
plot(rmse.cv,pch=19,type="b")
reg.best=regsubsets (lbwght~.,data=datos.train , nvmax=nvariables, method = "forward")
coef(reg.best ,which.min(rmse.cv))

# Modelo final acorde a las la mejor selección de conjuntos
fit.final <- lm(lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)
summary(fit.final)

regfit.full=regsubsets(lbwght~.,data= datos[train,],nvmax=nvariables, method = "forward")

lm.pred = predict.regsubsets(regfit.full, newdata = datos.test, id=which.min(rmse.cv))
error.mss <- mean((datos.test[, "lbwght"] - lm.pred)^2)
error.mss 

results["MCO with Forward step wise", "10 Cross Validation"] = error.mss
```

#### d)
```{r}
nvariables <- as.numeric(dim(datos)[2] - 1)
regfit.full = regsubsets(lbwght~., data=datos[train,], nvmax=nvariables)

predict.regsubsets=function(object, newdata, id, ...) {
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id) 
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

k = 5
set.seed(100)
folds=sample(1:k,nrow(datos.train),replace=TRUE)

table(folds)
cv.errors=matrix(NA,k,nvariables, dimnames =list(NULL , paste(1:nvariables)))
for(j in 1:k){
  best.fit=regsubsets(lbwght~.,data=datos.train[folds!=j,],
                      nvmax=nvariables)
  for(i in 1:nvariables){
    pred=predict.regsubsets(best.fit,datos.train[folds==j,],id=i)
    cv.errors[j,i]=mean( (datos.train$lbwght[folds==j]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
rmse.cv
which.min(rmse.cv)
plot(rmse.cv,pch=19,type="b")
reg.best=regsubsets (lbwght~.,data=datos.train , nvmax=nvariables)
coef(reg.best ,which.min(rmse.cv))

# Modelo final acorde a las la mejor selección de conjuntos
fit.final <- lm(lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)
summary(fit.final)

regfit.full=regsubsets(lbwght~.,data= datos[train,],nvmax=nvariables)

lm.pred = predict.regsubsets(regfit.full, newdata = datos.test, id=which.min(rmse.cv))
error.mss <- mean((datos.test[, "lbwght"] - lm.pred)^2)
error.mss 

results["MCO with Subset Selection", "5 Cross Validation"] = error.mss
```

```{r}
nvariables <- as.numeric(dim(datos)[2] - 1)
regfit.full = regsubsets(lbwght~., data=datos[train,], nvmax=nvariables, method = "forward")

k = 5
set.seed(100)
folds=sample(1:k,nrow(datos.train),replace=TRUE)

table(folds)
cv.errors=matrix(NA,k,nvariables, dimnames =list(NULL , paste(1:nvariables)))
for(j in 1:k){
  best.fit=regsubsets(lbwght~.,data=datos.train[folds!=j,],
                      nvmax=nvariables, method = "forward")
  for(i in 1:nvariables){
    pred=predict.regsubsets(best.fit,datos.train[folds==j,],id=i)
    cv.errors[j,i]=mean( (datos.train$lbwght[folds==j]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
rmse.cv
which.min(rmse.cv)
plot(rmse.cv,pch=19,type="b")
reg.best=regsubsets (lbwght~.,data=datos.train , nvmax=nvariables, method = "forward")
coef(reg.best ,which.min(rmse.cv))

# Modelo final acorde a las la mejor selección de conjuntos
fit.final <- lm(lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)
summary(fit.final)

regfit.full=regsubsets(lbwght~.,data= datos[train,],nvmax=nvariables, method = "forward")

lm.pred = predict.regsubsets(regfit.full, newdata = datos.test, id=which.min(rmse.cv))
error.mss <- mean((datos.test[, "lbwght"] - lm.pred)^2)
error.mss 

results["MCO with Forward step wise", "5 Cross Validation"] = error.mss
```

#### e)
A continuación se muestran los modelos utilizados anteriormente con el tipo de validación cruzada y su error de prueba correspondiente.
```{r}
kable(results)
```
Como se puede observar en la tabla no hay mucha diferencia entre los errores de prueba obtenidos de estos enfoques.

#### f)
Para seleccionar el modelo que minimiza el error, como no hay mucha diferencia entre los errores anteriores, escogimos el último modelo analizado **MCO with Forward step wise**. En el análisis de este modelo se determinaron las variables significativas al 5%, son las que se usan a continuación.

```{r}
fit.final <- lm(lbwght ~ bwght + fmaps + lbw + vlbw, data = datos.train)

lm.pred <- predict.regsubsets(fit.final, newdata = datos.test)
error.new =  mean((datos.test[, "lbwght"] - lm.pred)^2)
error.new
```

El modelo con sólo cuatro variables explicativas con un nivel de significación del 5% (**bwght, fmaps, lbw, vlbw**) y un error de `r error.mco`, es un poco mas preciso respecto al error del MCO: `r error.mco`.

Los **p-valores** se muestran a continuación:
```{r}
summary(fit.final)[4]
```

#### g)
Regresión de Ridge
```{r echo=FALSE}
results2 <- matrix(NA, nrow = 5, ncol = 2)
colnames(results2) <- c("5 Cross Validation","10 Cross Validation")
rownames(results2) <- c("RIDGE","LASSO","PCA", "PLS", "LASSO with Elastic Net")
results2 <- as.table(results2)
```

```{r}
set.seed(100)
library(glmnet)
x=model.matrix(lbwght~.,datos)[,-1]
y=datos$lbwght
y.test=y[test]
grid=10^seq(10,-2, length =100)
cv.ridge=cv.glmnet(x[train ,],y[train],alpha=0,lambda=grid)
plot(cv.ridge)
mejorlambda=cv.ridge$lambda.min
mejorlambda

ridge.mod=glmnet(x[train ,],y[train],alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,s=mejorlambda ,newx=x[test ,])

error.ridge <- mean((ridge.pred-datos.test[, "lbwght"] )^2)
error.ridge

results2["RIDGE", "10 Cross Validation"] = error.ridge

# Regla del "codo" de una DT del error de VC:
lambda.codo <- cv.ridge$lambda.1se
lambda.codo

ridge.pred.2=predict(ridge.mod,s=lambda.codo,newx=x[test ,])
error.ridge.2 <- mean((ridge.pred.2-datos.test[, "lbwght"] )^2)
error.ridge.2
```

#### h)
Modelo de LASSO
```{r}
set.seed(100)
cv.lasso=cv.glmnet(x[train ,],y[train],alpha=1, lambda = grid)
plot(cv.lasso)
bestlam=cv.lasso$lambda.min
bestlam

lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test ,])

error.lasso <- mean((lasso.pred-datos.test[, "lbwght"] )^2)
error.lasso
results2["LASSO", "10 Cross Validation"] = error.lasso

# Regla del "codo" de una DT del error de VC:
lambda.codo.l <- cv.lasso$lambda.1se
lambda.codo.l

lasso.pred.2=predict(lasso.mod,s=lambda.codo.l,newx=x[test ,])
error.lasso.2 <- mean((lasso.pred.2-datos.test[, "lbwght"] )^2)
error.lasso.2

```
Para obtener los coeficientes del modelo para el mejor lambda usamos la función coef
```{r}
c = coef(lasso.mod, s = bestlam)
c
```
Los coeficientes diferentes de 0 son `r length(c[c!=0]) - 1`.

#### i)

Regresión de Ridge con CV 5 veces

```{r}
set.seed(100)
library(glmnet)
x=model.matrix(lbwght~.,datos)[,-1]
y=datos$lbwght
y.test=y[test]
grid=10^seq(10,-2, length =100)
cv.ridge=cv.glmnet(x[train ,],y[train],alpha=0,lambda=grid, nfolds = 5)
plot(cv.ridge)
mejorlambda=cv.ridge$lambda.min
mejorlambda

ridge.mod=glmnet(x[train ,],y[train],alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,s=mejorlambda ,newx=x[test ,])

error.ridge <- mean((ridge.pred-datos.test[, "lbwght"] )^2)
error.ridge

results2["RIDGE", "5 Cross Validation"] = error.ridge

```

Modelo de LASSO
```{r}
set.seed(100)
cv.lasso=cv.glmnet(x[train ,],y[train],alpha=1, lambda = grid, nfolds = 5)
plot(cv.lasso)
bestlam=cv.lasso$lambda.min
bestlam

lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test ,])

error.lasso <- mean((lasso.pred-datos.test[, "lbwght"] )^2)
error.lasso

results2["LASSO", "5 Cross Validation"] = error.lasso

```
Para obtener los coeficientes del modelo para el mejor lambda usamos la función coef
```{r}
c = coef(lasso.mod, s = bestlam)
c
```
Los coeficientes diferentes de 0 son `r length(c[c!=0]) - 1`.

#### j)
```{r}
library(pls)
set.seed(100)
pcr.fit=pcr(lbwght~., data=datos,subset=train,scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP", xlab = "Número de Componentes Principales")
pcr.cv <- crossval(pcr.fit, segments = 10)
plot(RMSEP(pcr.cv), legendpos="topright")
summary(pcr.cv, what = "validation")

## Selecciona el número de componentes principales
## Regla del codo: 1 d.t.
ncomp.1.d.t. <- selectNcomp(pcr.fit, method = "onesigma", plot = TRUE, validation = "CV",
                            segments = 10)
ncomp.1.d.t.
pcr.pred=predict(pcr.fit,newdata=x[test,],ncomp=ncomp.1.d.t.)
error.pcr <- mean((pcr.pred - datos.test[, "lbwght"])^2)
error.pcr

results2["PCA", "10 Cross Validation"] = error.pcr
```
Usando el Metodo de Componentes Principales con una Validación Cruzada 10 veces obtenemos la cantidad de componentes M = `r ncomp.1.d.t.` y un error de prueba de `r error.pcr`.

El mismo método pero con la validación cruzada 5 veces seria:
```{r}
set.seed(100)
pcr.fit=pcr(lbwght~., data=datos,subset=train,scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP", xlab = "Número de Componentes Principales")
pcr.cv <- crossval(pcr.fit, segments = 5)
plot(RMSEP(pcr.cv), legendpos="topright")
summary(pcr.cv, what = "validation")

## Selecciona el número de componentes principales
## Regla del codo: 1 d.t.
ncomp.1.d.t. <- selectNcomp(pcr.fit, method = "onesigma", plot = TRUE, validation = "CV",
                            segments = 5)
ncomp.1.d.t.
pcr.pred=predict(pcr.fit,newdata=x[test,],ncomp=ncomp.1.d.t.)
error.pcr <- mean((pcr.pred - datos.test[, "lbwght"])^2)
error.pcr

results2["PCA", "5 Cross Validation"] = error.pcr
```
Usando el Metodo de Componentes Principales con una Validación Cruzada 5 veces obtenemos la cantidad de componentes M = `r ncomp.1.d.t.` y un error de prueba de `r error.pcr`.

#### k)
PLS con Validación cruzada 10 veces

```{r}
set.seed(100)
pls.fit=plsr(lbwght~., data=datos,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP", xlab = "Número de Componentes Principales")
pls.cv <- crossval(pls.fit, segments = 10)
plot(RMSEP(pls.cv), legendpos="topright")
summary(pls.cv, what = "validation")


## Utilizamos 4 componentes por el Mínimo Error de VC
pls.pred=predict(pls.fit,newdata=x[test,],ncomp=4)
error.pls <- mean((pls.pred - datos.test[, "lbwght"])^2)
error.pls

## Selecciona el número de componentes principales
## Regla del codo: 1 d.t.
ncomp.1.d.t. <- selectNcomp(pls.fit, method = "onesigma", plot = TRUE, validation = "CV",
                            segments = 10)
ncomp.1.d.t.
pls.pred.2=predict(pls.fit,newdata=x[test,],ncomp=ncomp.1.d.t.)
error.pls.codo <- mean((pls.pred.2 - datos.test[, "lbwght"])^2)
error.pls.codo

results2["PLS", "10 Cross Validation"] = error.pls.codo

## Regla de la permutación: se selecciona el ncomp que nos da el min Error de VC
ncomp.perm <- selectNcomp(pls.fit, method = "randomization", plot = TRUE)
ncomp.perm
pls.pred.3=predict(pls.fit,newdata=x[test,],ncomp=ncomp.perm)
error.pls.perm <- mean((pls.pred.3 - datos.test[, "lbwght"])^2)
error.pls.perm
```

PLS con Validación cruzada 5 veces

```{r}
set.seed(100)
pls.fit=plsr(lbwght~., data=datos,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP", xlab = "Número de Componentes Principales")
pls.cv <- crossval(pls.fit, segments = 5)
plot(RMSEP(pls.cv), legendpos="topright")
summary(pls.cv, what = "validation")


## Utilizamos 4 componentes por el Mínimo Error de VC
pls.pred=predict(pls.fit,newdata=x[test,],ncomp=4)
error.pls <- mean((pls.pred - datos.test[, "lbwght"])^2)
error.pls

## Selecciona el número de componentes principales
## Regla del codo: 1 d.t.
ncomp.1.d.t. <- selectNcomp(pls.fit, method = "onesigma", plot = TRUE, validation = "CV",
                            segments = 5)
ncomp.1.d.t.
pls.pred.2=predict(pls.fit,newdata=x[test,],ncomp=ncomp.1.d.t.)
error.pls.codo <- mean((pls.pred.2 - datos.test[, "lbwght"])^2)
error.pls.codo

results2["PLS", "5 Cross Validation"] = error.pls.codo

## Regla de la permutación: se selecciona el ncomp que nos da el min Error de VC
ncomp.perm <- selectNcomp(pls.fit, method = "randomization", plot = TRUE)
ncomp.perm
pls.pred.3=predict(pls.fit,newdata=x[test,],ncomp=ncomp.perm)
error.pls.perm <- mean((pls.pred.3 - datos.test[, "lbwght"])^2)
error.pls.perm
```

#### l)
Modelo LASSO con la restricción de Red Elástica (LASSO with Elastic Net) en el conjunto de
entrenamiento, con el α y el λ elegido mediante la Validación Cruzada 10-Veces. 

```{r}
library(glmnet)
library(caret)
set.seed(100)
lambda.grid <- 10^seq(2,-2, length = 100)
alpha.grid <- seq(0,1, by = 0.05)

Control <- trainControl(method = "cv", number = 10)
busca.grid <- expand.grid(alpha = alpha.grid, lambda = lambda.grid)

set.seed(100)
mi.entrenamiento <- train(lbwght~., data = datos.train, method = "glmnet", 
                          tuneGrid = busca.grid, trControl = Control,
                          tuneLength = 10,
                          standardize = TRUE, maxit = 1000000)

plot(mi.entrenamiento)
attributes(mi.entrenamiento)
mi.entrenamiento$bestTune

mi.modelo.glmnet <- mi.entrenamiento$finalModel
coef(mi.modelo.glmnet, s = mi.entrenamiento$bestTune$lambda)
mej.modelo <- glmnet(x[train ,],y[train], alpha=mi.entrenamiento$bestTune$alpha,
                     lambda = mi.entrenamiento$bestTune$lambda)
c = coef(mej.modelo, s = mi.entrenamiento$bestTune$lambda)
cbind(coef(mej.modelo, s = mi.entrenamiento$bestTune$lambda), coef(mi.modelo.glmnet, s = mi.entrenamiento$bestTune$lambda))

lre.pred <- predict(mej.modelo,s=mi.entrenamiento$bestTune$lambda,newx=x[test ,])

error.lassoelastic <- mean((lre.pred - datos.test[, "lbwght"])^2)
error.lassoelastic

results2["LASSO with Elastic Net", "10 Cross Validation"] = error.lassoelastic

```
El error de prueba obtenido es `r error.lassoelastic` y la cantidad de coeficientes distintos de 0 es `r length(c[c != 0]) -1`.


#### m)
Modelo LASSO con la restricción de Red Elástica (LASSO with Elastic Net) en el conjunto de
entrenamiento, con el α y el λ elegido mediante la Validación Cruzada 5-Veces. 

```{r}
set.seed(100)
lambda.grid <- 10^seq(2,-2, length = 100)
alpha.grid <- seq(0,1, by = 0.05)
Control <- trainControl(method = "cv", number = 5)
busca.grid <- expand.grid(alpha = alpha.grid, lambda = lambda.grid)
set.seed(100)
mi.entrenamiento <- train(lbwght~., data = datos.train, method = "glmnet", 
                          tuneGrid = busca.grid, trControl = Control,
                          tuneLength = 5,
                          standardize = TRUE, maxit = 1000000)

plot(mi.entrenamiento)
attributes(mi.entrenamiento)
mi.entrenamiento$bestTune

mi.modelo.glmnet <- mi.entrenamiento$finalModel
coef(mi.modelo.glmnet, s = mi.entrenamiento$bestTune$lambda)
mej.modelo <- glmnet(x[train ,],y[train], alpha=mi.entrenamiento$bestTune$alpha,
                     lambda = mi.entrenamiento$bestTune$lambda)
c = coef(mej.modelo, s = mi.entrenamiento$bestTune$lambda)
cbind(coef(mej.modelo, s = mi.entrenamiento$bestTune$lambda), coef(mi.modelo.glmnet, s = mi.entrenamiento$bestTune$lambda))

lre.pred <- predict(mej.modelo,s=mi.entrenamiento$bestTune$lambda,newx=x[test ,])

error.lassoelastic <- mean((lre.pred - datos.test[, "lbwght"])^2)
error.lassoelastic
results2["LASSO with Elastic Net", "5 Cross Validation"] = error.lassoelastic

```
El error de prueba obtenido es `r error.lassoelastic` y la cantidad de coeficientes distintos de 0 es `r length(c[c != 0]) -1`.


#### o)
Ajusta un modelo Rigorous LASSO (RLASSO) sobre el conjunto de entrenamiento con el lambda elegido mediante la penalización dependiente de los datos

```{r echo = F}
library(biglasso)
library(hdm)
set.seed(100)

# ajustamos los datos
x = datos.train
y = datos.train[, "lbwght"]
x[, "lbwght"] <- NULL;


lasso.reg.dep = rlasso(x = x, y = y, post=FALSE, X.dependent.lambda = TRUE) 

sum.lasso.dep <- summary(lasso.reg.dep, all=FALSE)
sum.lasso.dep
yhat.lasso.dep.new = predict(lasso.reg.dep, newdata=datos.test) #out-of-sample prediction
```

Informa el error de prueba obtenido junto con el número de coeficientes estimados diferentes de cero.

Total de variables = **20**
Variables seleccionadas = **1**
p-value = **0**

```{r echo = F}
error.rlasso.dep <- mean((yhat.lasso.dep.new - datos.test[, "lbwght"] )^2)
error.rlasso.dep
```
El error de prueba de Rigorous Lasso con penalización dependiente de los datos es `r error.rlasso.dep`

Con el lambda elegido mediante la penalización independiente de los datos

```{r echo = F}
set.seed(100)
lasso.reg.ind = rlasso(x = x, y = y, post=FALSE) # use lasso, not-Post-lasso
sum.lasso.ind <- summary(lasso.reg.ind, all=FALSE)
sum.lasso.ind
yhat.lasso.ind.new = predict(lasso.reg.ind, newdata=datos.test) #out-of-sample prediction

```
Informa el error de prueba obtenido junto con el número de coeficientes estimados diferentes de cero.

Total de variables = **20**
Variables seleccionadas = **1**
p-value = **0**

```{r echo = F}
error.rlasso.ind <- mean((yhat.lasso.ind.new - datos.test[, "lbwght"] )^2)
error.rlasso.ind
```
El error de prueba de Rigorous Lasso con penalización independiente de los datos es `r error.rlasso.ind`

Calcula el error de prueba de los dos modelos mediante el ajuste de Post-LASSO.

```{r echo = F}
set.seed(100)
post.lasso.reg = rlasso(x = x, y = y, post=TRUE) #now use post-lasso
sum.post.lasso.reg <- summary(post.lasso.reg, all=FALSE)
sum.post.lasso.reg
print(post.lasso.reg, all=FALSE) # or use summary(post.lasso.reg, all=FALSE)
yhat.postlasso.new = predict(post.lasso.reg, newdata=datos.test) #out-of-sample prediction

```
####Informa el error de prueba obtenido junto con el número de coeficientes estimados diferentes de cero.

Total de variables = **20**
Variables seleccionadas = **1**
p-value = **0**

```{r echo = F}
error.postlasso <- mean((yhat.postlasso.new - datos.test[, "lbwght"] )^2)
error.postlasso

```
El error de prueba Post-Lasso es `r error.postlasso`


#### p)

Contrastar la significación individual de los coeficientes estimados del modelo final. Utiliza el nivel de significación del 5%. 

```{r echo = F}
set.seed(100)
lasso.effect = rlassoEffects(x = x, y = y)
summary(lasso.effect)
plot(lasso.effect)
```
Sólo las variables ***bwght, fmaps, lbw y vlbw** son significativas sobre un nivel de confianza del 1%

####Eliminamos las variables no significativas

```{r echo = F}
x.nuevo.1 <- x[, -c(1:6,8,10:11,14:20)] 

lasso.effect.1 = rlassoEffects(x = x.nuevo.1, y = y)
summary(lasso.effect.1)

```
Las cuatro variables son muy significativas.

Informa el error de prueba obtenido
```{r echo = F}
set.seed(100)
lasso.reg.2 = rlasso(x = x.nuevo.1, y = y, post=FALSE)
yhat.lasso.new.2 = predict(lasso.reg.2, newdata=datos.test) 
error.rlasso.2 <- mean((yhat.lasso.new.2 - datos.test[, "lbwght"]  )^2)
error.rlasso.2
```
El error de prueba obtenido es `r error.rlasso.2`

Basándose en contrastes de significación individual, selecciona modelos que contengan sólo variables significativas al 5% de significación y estima el error de prueba de estos modelos (mediante el ajuste de Post-LASSO).

```{r echo = F}
set.seed(100)
post.lasso.reg.2 = rlasso(x = x.nuevo.1, y = y, post=TRUE) 
summary(post.lasso.reg.2, all=FALSE) 
yhat.postlasso.new.2 = predict(post.lasso.reg.2, newdata=datos.test) 
error.postlasso.2 <- mean((yhat.postlasso.new.2 - datos.test[, "lbwght"] )^2)
error.postlasso.2

```

El error de prueba de estos modelos es `r error.postlasso.2`


#### q)
A continuación se muestran los modelos utilizados anteriormente con el tipo de validación cruzada y su error de prueba correspondiente.
```{r}
kable(results2)
```

r. Haz una tabla con los coeficientes estimados con sus p-valores del modelo con el mejor ajuste. Comenta tus
resultados. Interpreta los coeficientes estimados del modelo con el mejor ajuste.

